services:
  app:
    build: 
      context: .
      dockerfile: Dockerfile
    container_name: stt-translation-app
    ports:
      - "8000:8000"
    environment:
      # Ollama is now on the same host (AI host)
      # host.docker.internal allows container to access host's Ollama service
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
      # FORCE override model to avoid OOM
      - OLLAMA_MODEL=gpt-oss:20b
      # Enable GPU for Faster Whisper and LayoutParser
      - FORCE_CPU=false
    volumes:
      - ./backend/app:/app/app
    restart: always
    # Enable host.docker.internal on Linux (Windows/Mac already support it)
    extra_hosts:
      - "host.docker.internal:host-gateway"
    # Enable GPU support
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]




